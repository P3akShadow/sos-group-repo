{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7cf75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pdcoding\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c286f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOMToolbox Parser\n",
    "from SOMToolBox_Parse import SOMToolBox_Parse\n",
    "idata = SOMToolBox_Parse(\"datasets/iris/iris.vec\").read_weight_file()\n",
    "weights = SOMToolBox_Parse(\"datasets/iris/iris.wgt.gz\").read_weight_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "259a4e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HitHistogram\n",
    "def HitHist(_m, _n, _weights, _idata):\n",
    "    hist = np.zeros(_m * _n)\n",
    "    for vector in _idata: \n",
    "        position =np.argmin(np.sqrt(np.sum(np.power(_weights - vector, 2), axis=1)))\n",
    "        hist[position] += 1\n",
    "\n",
    "    return hist.reshape(_m, _n)\n",
    "\n",
    "#U-Matrix - implementation\n",
    "def UMatrix(_m, _n, _weights, _dim):\n",
    "    U = _weights.reshape(_m, _n, _dim)\n",
    "    U = np.insert(U, np.arange(1, _n), values=0, axis=1)\n",
    "    U = np.insert(U, np.arange(1, _m), values=0, axis=0)\n",
    "    #calculate interpolation\n",
    "    for i in range(U.shape[0]): \n",
    "        if i%2==0:\n",
    "            for j in range(1,U.shape[1],2):\n",
    "                U[i,j][0] = np.linalg.norm(U[i,j-1] - U[i,j+1], axis=-1)\n",
    "        else:\n",
    "            for j in range(U.shape[1]):\n",
    "                if j%2==0: \n",
    "                    U[i,j][0] = np.linalg.norm(U[i-1,j] - U[i+1,j], axis=-1)\n",
    "                else:      \n",
    "                    U[i,j][0] = (np.linalg.norm(U[i-1,j-1] - U[i+1,j+1], axis=-1) + np.linalg.norm(U[i+1,j-1] - U[i-1,j+1], axis=-1))/(2*np.sqrt(2))\n",
    "\n",
    "    U = np.sum(U, axis=2) #move from Vector to Scalar\n",
    "\n",
    "    for i in range(0, U.shape[0], 2): #count new values\n",
    "        for j in range(0, U.shape[1], 2):\n",
    "            region = []\n",
    "            if j>0: region.append(U[i][j-1]) #check left border\n",
    "            if i>0: region.append(U[i-1][j]) #check bottom\n",
    "            if j<U.shape[1]-1: region.append(U[i][j+1]) #check right border\n",
    "            if i<U.shape[0]-1: region.append(U[i+1][j]) #check upper border\n",
    "\n",
    "            U[i,j] = np.median(region)\n",
    "\n",
    "    return U\n",
    "\n",
    "#SDH - implementation\n",
    "def SDH(_m, _n, _weights, _idata, factor, approach):\n",
    "    import heapq\n",
    "\n",
    "    sdh_m = np.zeros( _m * _n)\n",
    "\n",
    "    cs=0\n",
    "    for i in range(factor): cs += factor-i\n",
    "\n",
    "    for vector in _idata:\n",
    "        dist = np.sqrt(np.sum(np.power(_weights - vector, 2), axis=1))\n",
    "        c = heapq.nsmallest(factor, range(len(dist)), key=dist.__getitem__)\n",
    "        if (approach==0): # normalized\n",
    "            for j in range(factor):  sdh_m[c[j]] += (factor-j)/cs \n",
    "        if (approach==1):# based on distance\n",
    "            for j in range(factor): sdh_m[c[j]] += 1.0/dist[c[j]] \n",
    "        if (approach==2): \n",
    "            dmin, dmax = min(dist[c]), max(dist[c])\n",
    "            for j in range(factor): sdh_m[c[j]] += 1.0 - (dist[c[j]]-dmin)/(dmax-dmin)\n",
    "\n",
    "    return sdh_m.reshape(_m, _n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c32c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "hv.extension('bokeh')\n",
    "\n",
    "hithist = hv.Image(HitHist(weights['ydim'], weights['ydim'], weights['arr'], idata['arr'])).opts(xaxis=None, yaxis=None) \n",
    "um = hv.Image(UMatrix(weights['ydim'], weights['ydim'], weights['arr'], 4)).opts(xaxis=None, yaxis=None) \n",
    "sdh = hv.Image(SDH(weights['ydim'], weights['ydim'], weights['arr'], idata['arr'], 25, 0)).opts(xaxis=None, yaxis=None)   \n",
    "\n",
    "hv.Layout([hithist.relabel('HitHist').opts(cmap='kr'), \n",
    "           um.relabel('U-Matrix').opts(cmap='jet'), sdh.relabel('SDH').opts(cmap='viridis')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "644651b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minisom\n",
    "\n",
    "class AlignedSomLayer(minisom.MiniSom):\n",
    "    def __init__(self, n, i, featuresA, x, y, input_len, sigma=1.0, learning_rate=0.5,\n",
    "                 decay_function=minisom.asymptotic_decay,\n",
    "                 neighborhood_function='gaussian', topology='rectangular',\n",
    "                 activation_distance='euclidean', random_seed=42):\n",
    "        super().__init__(x, y, input_len, sigma, learning_rate, decay_function, neighborhood_function, topology, activation_distance, random_seed)\n",
    "        self.n = n\n",
    "        self.i = i\n",
    "        self.featuresA = featuresA\n",
    "        \n",
    "        self.underyling_activation_function = self._activation_distance\n",
    "        self._activation_distance = self.activation_scale_wrapper\n",
    "        \n",
    "        #self.rescale(featuresA, n, i)\n",
    "\n",
    "    def activation_scale_wrapper(self, x, w):\n",
    "        # print(x)\n",
    "        n = self.n\n",
    "        i = self.i\n",
    "        scaleA = (n-i-1)/(n-1)\n",
    "        scaleB = i/(n-1)\n",
    "\n",
    "        factor = []\n",
    "        for j in range(self._input_len):\n",
    "            if j in self.featuresA:\n",
    "                factor.append(scaleA)\n",
    "            else:\n",
    "                factor.append(scaleB)\n",
    "        \n",
    "        xcopy = x.copy()\n",
    "        wcopy = w.copy()\n",
    "\n",
    "        # print(f\"layer {i}\")\n",
    "        # print(\"old\")\n",
    "        # print(xcopy)\n",
    "        # print(wcopy)\n",
    "\n",
    "        xcopy *= np.array(factor)\n",
    "        wcopy *= np.array(factor)\n",
    "\n",
    "\n",
    "        # print(\"new\")\n",
    "        # print(xcopy)\n",
    "        # print(wcopy)\n",
    "\n",
    "        return self.underyling_activation_function(xcopy, wcopy)\n",
    "\n",
    "\n",
    "\n",
    "    def rescale(self, featuresA, n, i):\n",
    "        scaleA = (n-i-1)/(n-1)\n",
    "        scaleB = i/(n-1)\n",
    "\n",
    "        factor = []\n",
    "        for i in range(self._input_len):\n",
    "            if i in featuresA:\n",
    "                factor.append(scaleA)\n",
    "            else:\n",
    "                factor.append(scaleB)\n",
    "\n",
    "        self._weights *= factor\n",
    "\n",
    "\n",
    "    def update_scaled(self, x, win, t, max_iteration, scaling_factor):\n",
    "        \"\"\"\n",
    "        Updates the weights of the neurons. Scaled by the scaling Factor.\n",
    "        In practice, this factor can be something like 1 / how far the layer is from the \"pivot\" layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            Current pattern to learn.\n",
    "        win : tuple\n",
    "            Position of the winning neuron for x (array or tuple).\n",
    "        t : int\n",
    "            rate of decay for sigma and learning rate\n",
    "        max_iteration : int\n",
    "            If use_epochs is True:\n",
    "                Number of epochs the SOM will be trained for\n",
    "            If use_epochs is False:\n",
    "                Maximum number of iterations (one iteration per sample).\n",
    "        scaling_factor: float\n",
    "            factor the update is scaled by\n",
    "        \"\"\"\n",
    "        eta = self._decay_function(self._learning_rate, t, max_iteration)\n",
    "        # sigma and learning rate decrease with the same rule\n",
    "        sig = self._decay_function(self._sigma, t, max_iteration)\n",
    "        # improves the performances\n",
    "        g = self.neighborhood(win, sig)*eta*scaling_factor\n",
    "        # w_new = eta * neighborhood_function * (x-w)\n",
    "        self._weights += np.einsum('ij, ijk->ijk', g, x-self._weights)\n",
    "\n",
    "    def update_3d(self, x, win, t, max_iteration, chosen_layer_index):\n",
    "        \"\"\"Updates the weights of the neurons.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            Current pattern to learn.\n",
    "        win : tuple\n",
    "            Position of the winning neuron for x (array or tuple).\n",
    "        t : int\n",
    "            rate of decay for sigma and learning rate\n",
    "        max_iteration : int\n",
    "            If use_epochs is True:\n",
    "                Number of epochs the SOM will be trained for\n",
    "            If use_epochs is False:\n",
    "                Maximum number of iterations (one iteration per sample).\n",
    "        \"\"\"\n",
    "        win_3d = win\n",
    "\n",
    "\n",
    "        eta = self._decay_function(self._learning_rate, t, max_iteration)\n",
    "        # sigma and learning rate decrease with the same rule\n",
    "        sig = self._decay_function(self._sigma, t, max_iteration)\n",
    "        # improves the performances\n",
    "        g = self.neighborhood(win, sig)*eta\n",
    "        # w_new = eta * neighborhood_function * (x-w)\n",
    "        self._weights += np.einsum('ij, ijk->ijk', g, x-self._weights)\n",
    "\n",
    "    # def _cosine_distance(self, x, w):\n",
    "    #     num = (w * x).sum(axis=2)\n",
    "    #     denum = multiply(linalg.norm(w, axis=2), linalg.norm(x))\n",
    "    #     return 1 - num / (denum+1e-8)\n",
    "\n",
    "    # def _euclidean_distance(self, x, w):\n",
    "    #     return linalg.norm(subtract(x, w), axis=-1)\n",
    "\n",
    "    # def _manhattan_distance(self, x, w):\n",
    "    #     return linalg.norm(subtract(x, w), ord=1, axis=-1)\n",
    "\n",
    "    # def _chebyshev_distance(self, x, w):\n",
    "    #     return max(subtract(x, w), axis=-1)\n",
    "\n",
    "\n",
    "# import minisom\n",
    "class AlignedSom:\n",
    "    def __init__(self, x, y, input_len, n, featuresA, sigma=1.0, learning_rate=0.5,\n",
    "                 decay_function=minisom.asymptotic_decay,\n",
    "                 neighborhood_function='gaussian', topology='rectangular',\n",
    "                 activation_distance='euclidean', random_seed=42):\n",
    "        \"\"\"\n",
    "        initializes the Aligned Som class.\n",
    "        n guides how many maps are generated\n",
    "        featuresA sets which features are in the A set (The rest is in B). This should be a list of indices < input_len\n",
    "\n",
    "        All the other params get handed over to the \"children soms\"\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.featuresA = featuresA\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self._random_seed = random_seed\n",
    "\n",
    "        self._learning_rate = learning_rate\n",
    "        self._sigma = sigma\n",
    "        self._input_len = input_len\n",
    "\n",
    "        self.topology = topology\n",
    "\n",
    "        self._decay_function = decay_function\n",
    "        \n",
    "        self._random_generator = np.random.RandomState(random_seed)\n",
    "\n",
    "        self._layers = [AlignedSomLayer(n, i, featuresA, x, y, input_len, sigma=sigma, learning_rate=learning_rate, \n",
    "                                        decay_function=decay_function, \n",
    "                                        neighborhood_function=neighborhood_function, topology=topology, \n",
    "                                        activation_distance=activation_distance, random_seed=random_seed\n",
    "                                        ) for i in range(n)]\n",
    "        \n",
    "        self._weights = [self._layers[i]._weights for i in range(n)]\n",
    "\n",
    "    def get_scaled_vector(self, x, i):\n",
    "        scaleA = (self.n - 1 - i)/(self.n-1)\n",
    "        scaleB = i/(self.n-1)\n",
    "\n",
    "        xnew = x.copy()\n",
    "        \n",
    "        for j in range(len(x)):\n",
    "            if j in self.featuresA:\n",
    "                xnew[j] *= scaleA\n",
    "            else:\n",
    "                xnew[j] *= scaleB\n",
    "        \n",
    "        return xnew\n",
    "\n",
    "    def train(self, data, num_iteration,\n",
    "              random_order=False, verbose=False, use_epochs=False):\n",
    "        \"\"\"\n",
    "        Trains the SOM.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.array or list\n",
    "            Data matrix.\n",
    "\n",
    "        num_iteration : int\n",
    "            If use_epochs is False, the weights will be\n",
    "            updated num_iteration times. Otherwise they will be updated\n",
    "            len(data)*num_iteration times.\n",
    "\n",
    "        random_order : bool (default=False)\n",
    "            If True, samples are picked in random order.\n",
    "            Otherwise the samples are picked sequentially.\n",
    "\n",
    "        verbose : bool (default=False)\n",
    "            If True the status of the training will be\n",
    "            printed each time the weights are updated.\n",
    "\n",
    "        use_epochs : bool (default=False)\n",
    "            If True the SOM will be trained for num_iteration epochs.\n",
    "            In one epoch the weights are updated len(data) times and\n",
    "            the learning rate is constat throughout a single epoch.\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "        # self._check_iteration_number(num_iteration)\n",
    "        # self._check_input_len(data)\n",
    "\n",
    "        random_generator = None\n",
    "        if random_order:\n",
    "            random_generator = self._random_generator\n",
    "        iterations = minisom._build_iteration_indexes(len(data), num_iteration,\n",
    "                                              verbose, random_generator,\n",
    "                                              use_epochs)\n",
    "        if use_epochs:\n",
    "            def get_decay_rate(iteration_index, data_len):\n",
    "                return int(iteration_index / data_len)\n",
    "        else:\n",
    "            def get_decay_rate(iteration_index, data_len):\n",
    "                return int(iteration_index)\n",
    "\n",
    "        for t, iteration in enumerate(iterations):\n",
    "            chosen_layer_index = self._random_generator.randint(low=0, high=self.n)\n",
    "            decay_rate = get_decay_rate(t, len(data))\n",
    "            \n",
    "            cur_training_sample = data[iteration]\n",
    "            winner = self._layers[chosen_layer_index].winner(cur_training_sample)\n",
    "\n",
    "\n",
    "            for update_layer_index in range(self.n):\n",
    "                diff = chosen_layer_index - update_layer_index\n",
    "                if diff < 0:\n",
    "                    diff = -diff\n",
    "                \n",
    "                scaling_factor = 1/(1+diff)\n",
    "                self._layers[update_layer_index].update_scaled(cur_training_sample, winner,\n",
    "                        decay_rate, num_iteration, scaling_factor)\n",
    "                # self._layers[update_layer_index].update_3d(cur_training_sample, winner,\n",
    "                #         decay_rate, num_iteration, chosen_layer_index)\n",
    "                \n",
    "        \n",
    "        # for t, iteration in enumerate(iterations):\n",
    "        #     decay_rate = get_decay_rate(t, len(data))\n",
    "        #     self.update(data[iteration], self.winner(data[iteration]),\n",
    "        #                 decay_rate, num_iteration)\n",
    "        if verbose:\n",
    "            print('\\n quantization error:', self.quantization_error(data))\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a00733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignedSom = AlignedSom(2, 2, 10, 11, [0, 1, 2])\n",
    "\n",
    "data = [[1]*10,[2]*10]\n",
    "alignedSom.train(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f875a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import panel as pn\n",
    "# import holoviews as hv\n",
    "# from holoviews import opts\n",
    "# hv.extension('bokeh')\n",
    "\n",
    "class AlignedSomVis():\n",
    "    def __init__(self, weights, chosen_visulization_index=0, input_data=None):\n",
    "        self._weights = weights\n",
    "        self._idata = input_data\n",
    "\n",
    "        self._num_layers = len(self._weights)\n",
    "        # images = [hv.Image(chosen_visulization()) for i in range(self._num_layers)]\n",
    "        xdim = self._weights[0].shape[0]\n",
    "        ydim = self._weights[0].shape[1]\n",
    "        vector_dim = len(self._idata[0])\n",
    "        # print(vector_dim)\n",
    "        self._images_HitHist = [hv.Image(HitHist(xdim, ydim, self._weights[i], self._idata)).opts(xaxis=None, yaxis=None) for i in range(self._num_layers)]\n",
    "        self._images_UMatrix = [hv.Image(UMatrix(xdim, ydim, self._weights[i], vector_dim)).opts(xaxis=None, yaxis=None) for i in range(self._num_layers)]\n",
    "        # sdh_paramd = [lambda factor, approach: SDH(xdim, ydim, self._weights[i], self._idata, factor, approach) for i in range(self._num_layers)]\n",
    "        # # self._maps_SDH[0] =  hv.HoloMap({(factor, approach): sdh_paramd[0](factor, approach) for factor in [i+1 for i in range(100)] for approach in [0, 1, 2]},  kdims=['factor', 'approach'])\n",
    "        # self._maps_SDH[0] =  hv.HoloMap({(factor, approach): SDH(xdim, ydim, self._weights[0], self._idata, factor, approach) for factor in [i+1 for i in range(100)] for approach in [0, 1, 2]},  kdims=['factor', 'approach'])\n",
    "        # for i in range(1, self._num_layers):\n",
    "        #     self._maps_SDH[i] = hv.DynamicMap(sdh_paramd[i], kdims=['factor', 'approach'])\n",
    "        # for dmap in self._maps_SDH:\n",
    "        #     dmap.redim.values(factor=[i+1 for i in range(100)], approach=[0, 1, 2])\n",
    "\n",
    "        # hithist = hv.Image(HitHist(weights['ydim'], weights['ydim'], weights['arr'], idata['arr'])).opts(xaxis=None, yaxis=None) \n",
    "        # um = hv.Image(UMatrix(weights['ydim'], weights['ydim'], weights['arr'], 4)).opts(xaxis=None, yaxis=None) \n",
    "        # sdh = hv.Image(SDH(weights['ydim'], weights['ydim'], weights['arr'], idata['arr'], 25, 0)).opts(xaxis=None, yaxis=None)   \n",
    "\n",
    "        # hv.Layout([hithist.relabel('HitHist').opts(cmap='kr'), \n",
    "        #         um.relabel('U-Matrix').opts(cmap='jet'), sdh.relabel('SDH').opts(cmap='viridis')])\n",
    "\n",
    "        self._visualizations = [self._images_HitHist, self._images_UMatrix]\n",
    "        self._default_color_map = ['kr', 'jet', 'viridis']\n",
    "\n",
    "        implemented_vis_number = 2\n",
    "\n",
    "    # def show(self):\n",
    "        # self._mainview = hv.Layout([self._images_HitHist[i].relabel(f'Layer {i}').opts(cmap='kr') for i in range(self._num_layers)])\n",
    "        self._available_visulizations = [hv.Layout([self._visualizations[j][i].relabel(f'Layer {i}').opts(cmap=self._default_color_map[j]) for i in range(self._num_layers)]) for j in range(implemented_vis_number)]\n",
    "        self._mainview = self._available_visulizations[chosen_visulization_index]\n",
    "        # self._mainview = sum([self._images[i].relabel(f'Layer {i}').opts(cmap='kr') for i in range(self._num_layers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7831e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pysomvis import PySOMVis, OBJECTS_CLASSES\n",
    "# # import pysomvis\n",
    "# from visualizations.complane import ComponentPlane\n",
    "# from visualizations.dmatrix import DMatrix\n",
    "# from visualizations.hithistogram import HitHist\n",
    "# from visualizations.sdh import SDH\n",
    "# from visualizations.qerror import QError\n",
    "# from visualizations.umatrix import UMatrix\n",
    "# from visualizations.upmatrix import UStar_PMatrix\n",
    "# from visualizations.neighbourhood_graph import NeighbourhoodGraph\n",
    "# from visualizations.clustering import Clustering\n",
    "# from visualizations.metromap import MetroMap\n",
    "# from visualizations.piechart import PieChart\n",
    "# from visualizations.chessboard import Chessboard\n",
    "# from visualizations.somstreamvis import SOMStreamVis\n",
    "# from visualizations.sky_metaphor import SkyMetaphor\n",
    "# from visualizations.topographic_error import TopographicError\n",
    "# from visualizations.intrinsic_distance import IntrinsicDistance\n",
    "# from visualizations.activityhist import ActivityHist\n",
    "# from visualizations.minimumSpanningTree import MinimumSpanningTree\n",
    "# from visualizations.cluster_connection import ClusterConnection\n",
    "# from mnemonics.mnemonicSOM import MnemonicSOM\n",
    "\n",
    "# from holoviews.streams import Pipe, Buffer\n",
    "# from controls.controllers import MainController, PointOptions, SegmentOptions\n",
    "\n",
    "# class AlignedSomVisFirstAttempt(PySOMVis):\n",
    "\n",
    "\n",
    "#     def __init__(self, weights, m=None, n=None, dimension=None, input_data=None, classes_names=None, classes=None, component_names=None):\n",
    "        \n",
    "#         self._height = self._width = 500\n",
    "#         self._pipe = Pipe(data=[])\n",
    "#         self._pipe_points = Pipe(data=[])\n",
    "#         self._pipe_paths = Pipe(data=[])\n",
    "#         self._visualizations = []\n",
    "\n",
    "#         self._weights = weights\n",
    "#         #check ratio of the input map\n",
    "#         if len(self._weights[0].shape)==3 and m==None and n==None and dimension==None:\n",
    "#             self._m = self._weights[0].shape[0]\n",
    "#             self._n = self._weights[0].shape[1]\n",
    "#             self._dim = self._weights[0].shape[2]\n",
    "#             for layer_index in range(len(self._weights)):\n",
    "#                 self._weights[layer_index] = self._weights[layer_index].reshape(-1, self._dim)\n",
    "#         else:\n",
    "#             self._m = m\n",
    "#             self._n = n\n",
    "#             self._dim = dimension\n",
    "\n",
    "#         self._idata = input_data\n",
    "        \n",
    "#         if input_data is not None:\n",
    "#             self._distance = np.linalg.norm(self._idata[:, None, :] - self._idata[None, :, :], axis=-1)\n",
    "        \n",
    "#         if classes is not None: self._classes = classes.astype(int) \n",
    "#         else:       self._classes = classes\n",
    "#         if component_names is not None: self._component_names = component_names\n",
    "#         else:                           self._component_names = None\n",
    "#         if classes_names is not None: self._classes_names = classes_names\n",
    "#         else:                         self._classes_names = None\n",
    "\n",
    "\n",
    "#         self._plot = None\n",
    "#         self._maincontrol = MainController(self._interpolation, self._rotate, self._flip, self._visualizations, OBJECTS_CLASSES, name='') #TODO: n would also be lovely, maybe A/B\n",
    "#         self._pointoptions = PointOptions(name=\"Points\")        \n",
    "#         self._segmentoptions = SegmentOptions(name=\"Segments\")\n",
    "#         self._point_segment_options = pn.Tabs(self._pointoptions, self._segmentoptions)\n",
    "#         self._mainp = pn.Column(pn.panel(self._maincontrol, default_layout=pn.Row, width=700))\n",
    "\n",
    "#         self._xlim = (-.5*self._m/self._n,.5*self._m/self._n) if self._m>self._n else (-.5,.5)\n",
    "#         self._ylim = (-.5*self._n/self._m,.5*self._n/self._m) if self._n>self._m else (-.5,.5)\n",
    "#         #_COLOURS_93\n",
    "#         self._Image = hv.DynamicMap(hv.Image, streams=[self._pipe]).apply.opts(cmap=self._maincontrol.param.colormap, \n",
    "#             width=self._width, height=self._height, xlim=self._xlim, ylim=self._ylim)\n",
    "#         self._Paths = hv.DynamicMap(hv.Segments, streams=[self._pipe_paths]).apply.opts(alpha='alpha', line_width=self._segmentoptions.param.size, \n",
    "#                                                                                                                     color=self._segmentoptions.param.color)\n",
    "#         self._Points = hv.DynamicMap(hv.Points, streams=[self._pipe_points]).apply.opts(size=self._pointoptions.param.size, color=self._pointoptions.param.color,\n",
    "#                                                                                                                     marker=self._pointoptions.param.marker)\n",
    "        \n",
    "#         self._pdmap = pn.Column(self._Image * self._Paths * self._Points)\n",
    "\n",
    "#         self._controls = pn.Row()\n",
    "#         self._somstreamvis = pn.Row()\n",
    "#         self._mainview = pn.Column(pn.Column(self._mainp, pn.Row(self._pdmap, self._controls)), pn.Column(self._somstreamvis))\n",
    "       \n",
    "#         self._visualizations.append(ComponentPlane(self))\n",
    "#         if input_data is not None: self._visualizations.append(HitHist(self))\n",
    "#         self._visualizations.append(UMatrix(self))\n",
    "#         self._visualizations.append(DMatrix(self))\n",
    "#         if input_data is not None:\n",
    "#             self._visualizations.append(UStar_PMatrix(self))\n",
    "#             self._visualizations.append(SDH(self))\n",
    "#             self._visualizations.append(PieChart(self))\n",
    "#             self._visualizations.append(NeighbourhoodGraph(self))\n",
    "#             self._visualizations.append(Chessboard(self))\n",
    "#             self._visualizations.append(Clustering(self))\n",
    "#             self._visualizations.append(MetroMap(self))\n",
    "#             self._visualizations.append(QError(self))\n",
    "#             self._visualizations.append(SOMStreamVis(self))     \n",
    "#             self._visualizations.append(SkyMetaphor(self)) \n",
    "#             self._visualizations.append(TopographicError(self)) \n",
    "#             self._visualizations.append(IntrinsicDistance(self)) \n",
    "#             self._visualizations.append(ActivityHist(self))\n",
    "\n",
    "#         self._visualizations.append(MinimumSpanningTree(self))\n",
    "#         self._visualizations.append(ClusterConnection(self))\n",
    "#         self._visualizations.append(MnemonicSOM(self))\n",
    "#         self._visualizations[0]._activate_controllers()\n",
    "    \n",
    "#     def _rotate(self, k):\n",
    "#         for layer_index in range(len(self._weights)):\n",
    "#             self._weights[layer_index] = np.rot90(self._weights[layer_index].reshape(self._m, self._n, self._dim), k).reshape(-1,self._dim)\n",
    "#         self._pipe.send(np.rot90(self._pipe.data, k)) #TODO: this also in loop?\n",
    "#         if self._m != self._n: \n",
    "#             self._m, self._n = self._n, self._m\n",
    "#             self._ylim, self._xlim = self._xlim, self._ylim\n",
    "#             self._pdmap[0] = pn.Column(self._Image.opts(xlim=self._xlim, ylim=self._ylim) * self._Points * self._Paths)\n",
    "\n",
    "#     def _flip(self, horizontal):\n",
    "#         if horizontal:\n",
    "#             for layer_index in range(len(self._weights)):\n",
    "#                 self._weights[layer_index] = np.fliplr(self._weights[layer_index].reshape(self._m, self._n, self._dim)).reshape(-1,self._dim)\n",
    "#             self._pipe.send(np.fliplr(self._pipe.data)) #TODO: this also in loop?\n",
    "#         else:\n",
    "#             for layer_index in range(len(self._weights)):\n",
    "#                 self._weights[layer_index] = np.flipud(self._weights[layer_index].reshape(self._m, self._n, self._dim)).reshape(-1,self._dim)\n",
    "#             self._pipe.send(np.flipud(self._pipe.data)) #TODO: this also in loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7031b2f",
   "metadata": {},
   "source": [
    "C) Evaluation Report\n",
    "1) Perform and document (!) the testing of the components you coded by defining and\n",
    "evaluating suitable tests to evaluate the correctness and robustness of the coded modules.\n",
    "\n",
    "2) For systematic evaluation of tasks a-h, pick the Chainlink Data Set and the 10-Clusters\n",
    "dataset from\n",
    "http://www.ifs.tuwien.ac.at/dm/somtoolbox/datasets.html \t\t\t\t\t\t--> already available in pysomvis datasets\n",
    "\n",
    "3) Train a 10x10 (small) and a 100x60 (large) SOM. Make sure that the SOMs are properly\n",
    "trained, i.e. that the structures to be expected in the SOM become clearly visible by identifying\n",
    "suitable parameters for the initial neighborhood radius and initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d73aec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here\n",
    "from minisom import MiniSom\n",
    "# from pysomvis import PySOMVis #\n",
    "\n",
    "idata_chainlink   = SOMToolBox_Parse(\"datasets/chainlink/chainlink.vec\").read_weight_file()\n",
    "idata_10clusters   = SOMToolBox_Parse(\"datasets/10clusters/10clusters.vec\").read_weight_file()\n",
    "\n",
    "A_set_chainlink = [0, 1]\n",
    "A_set_10clusters = [0, 2, 4, 6, 8]\n",
    "\n",
    "num_layers_chainlink = 10\n",
    "num_layers_10clusters = 10\n",
    "\n",
    "small_som_chainlink = AlignedSom(10, 10, 3, num_layers_chainlink, A_set_chainlink, sigma=7, learning_rate=0.7)\n",
    "# small_som_chainlink = MiniSom(10, 10, 3, sigma=7, learning_rate=0.7)\n",
    "small_som_chainlink.train(idata_chainlink['arr'], 10000)\n",
    "# large_som_chainlink = MiniSom(100, 60, 3, sigma=7, learning_rate=0.7)\n",
    "# large_som_chainlink.train(idata_chainlink['arr'], 10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8036d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = AlignedSomVis(weights=small_som_chainlink._weights, input_data=idata_chainlink['arr'])\n",
    "vis._mainview\n",
    "vis._available_visulizations[1]\n",
    "# vis = AlignedSomVisFirstAttempt(weights=small_som_chainlink._weights, input_data=idata_chainlink['arr'])\n",
    "# vis._mainview\n",
    "# vis = PySOMVis(weights=small_som_chainlink._weights, input_data=idata_chainlink['arr'])\n",
    "# vis._mainview\n",
    "# vis = PySOMVis(weights=large_som_chainlink._weights, input_data=idata_chainlink['arr'])\n",
    "# vis._mainview\n",
    "\n",
    "# small_som_10clusters = MiniSom(10, 10, 10, learning_rate=0.7)\n",
    "# small_som_10clusters.train(idata_10clusters['arr'], 1000)\n",
    "# large_som_10clusters = MiniSom(100, 60, 10, learning_rate=0.7)\n",
    "# large_som_10clusters.train(idata_10clusters['arr'], 1000)\n",
    "\n",
    "# test_n = 2\n",
    "# viss = [PySOMVis(weights=small_som_chainlink._weights[i], input_data=idata_chainlink['arr']) for i in range(test_n)]\n",
    "# for i in range(1, test_n):\n",
    "#     viss[i]._mainp = viss[0]._mainp\n",
    "#     viss[i]._controls = viss[0]._controls\n",
    "\n",
    "# # vis0 = PySOMVis(weights=small_som_chainlink._weights[0], input_data=idata_chainlink['arr'])\n",
    "# # vis1 = PySOMVis(weights=small_som_chainlink._weights[1], input_data=idata_chainlink['arr'])\n",
    "# # vis0._pdmap + vis1._somstreamvis\n",
    "# # hv.Layout(viss[i]._mainview for i in range(test_n))\n",
    "# viss[0]._mainview + viss[1]._mainview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85af3ed7",
   "metadata": {},
   "source": [
    "4) Show the visualizations, providing examples with different parameter settings and\n",
    "comparisons that allow a validation of the correctness of the implementation. Specifically,\n",
    "test a few extreme values for the parameter settings.\n",
    "\n",
    "5) Where an identical visualization exists in the JÃVA SOM toolbox, read a SOM pre-trained\n",
    "with the JAVA SOM Toolbox (import functions are provided in the notebook) and compare\n",
    "your visualization with the one produced by the Java SOMToolbox (using either the pre\n",
    "trained SOMs provided with the toolbox, or any that your colleagues who do the analytics\n",
    "option of the exercise share with you). --> aligned SOM not part of JAVA SOM Toolbox\n",
    "\n",
    "6) Provide (export/print) the notebook as separate PDF report that comprises all information.\n",
    "Hence, the PDF export of the report needs to contain the fully-computed notebook with the\n",
    "according visualizations shown as results and the information that can be derived from the\n",
    "visualizations clearly described and semantically analyzed. Make sure that each visualization\n",
    "includes the parameter setting applied. Specifically, the PDF export needs to contain:\n",
    "- the implementation developed, explaining key parts of the code of each cell.\n",
    "- the way the code was systematically tested for correctness, including the test cases\n",
    "as part of the notebook.\n",
    "- the evaluations performed under item 3) above, demonstrating the correctness of the\n",
    "implementation, and the information gained.\n",
    "- Where applicable: Comparison of the visualization with the identical visualizations\n",
    "(reading the same trained SOM files) using the SOM Java Toolbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53774817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#probably some more code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOS_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
