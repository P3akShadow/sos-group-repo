{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7cf75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pdcoding\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c286f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOMToolbox Parser\n",
    "from SOMToolBox_Parse import SOMToolBox_Parse\n",
    "idata = SOMToolBox_Parse(\"datasets/iris/iris.vec\").read_weight_file()\n",
    "weights = SOMToolBox_Parse(\"datasets/iris/iris.wgt.gz\").read_weight_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259a4e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HitHistogram\n",
    "def HitHist(_m, _n, _weights, _idata):\n",
    "    hist = np.zeros(_m * _n)\n",
    "    for vector in _idata: \n",
    "        position =np.argmin(np.sqrt(np.sum(np.power(_weights - vector, 2), axis=1)))\n",
    "        hist[position] += 1\n",
    "\n",
    "    return hist.reshape(_m, _n)\n",
    "\n",
    "#U-Matrix - implementation\n",
    "def UMatrix(_m, _n, _weights, _dim):\n",
    "    U = _weights.reshape(_m, _n, _dim)\n",
    "    U = np.insert(U, np.arange(1, _n), values=0, axis=1)\n",
    "    U = np.insert(U, np.arange(1, _m), values=0, axis=0)\n",
    "    #calculate interpolation\n",
    "    for i in range(U.shape[0]): \n",
    "        if i%2==0:\n",
    "            for j in range(1,U.shape[1],2):\n",
    "                U[i,j][0] = np.linalg.norm(U[i,j-1] - U[i,j+1], axis=-1)\n",
    "        else:\n",
    "            for j in range(U.shape[1]):\n",
    "                if j%2==0: \n",
    "                    U[i,j][0] = np.linalg.norm(U[i-1,j] - U[i+1,j], axis=-1)\n",
    "                else:      \n",
    "                    U[i,j][0] = (np.linalg.norm(U[i-1,j-1] - U[i+1,j+1], axis=-1) + np.linalg.norm(U[i+1,j-1] - U[i-1,j+1], axis=-1))/(2*np.sqrt(2))\n",
    "\n",
    "    U = np.sum(U, axis=2) #move from Vector to Scalar\n",
    "\n",
    "    for i in range(0, U.shape[0], 2): #count new values\n",
    "        for j in range(0, U.shape[1], 2):\n",
    "            region = []\n",
    "            if j>0: region.append(U[i][j-1]) #check left border\n",
    "            if i>0: region.append(U[i-1][j]) #check bottom\n",
    "            if j<U.shape[1]-1: region.append(U[i][j+1]) #check right border\n",
    "            if i<U.shape[0]-1: region.append(U[i+1][j]) #check upper border\n",
    "\n",
    "            U[i,j] = np.median(region)\n",
    "\n",
    "    return U\n",
    "\n",
    "#SDH - implementation\n",
    "def SDH(_m, _n, _weights, _idata, factor, approach):\n",
    "    import heapq\n",
    "\n",
    "    sdh_m = np.zeros( _m * _n)\n",
    "\n",
    "    cs=0\n",
    "    for i in range(factor): cs += factor-i\n",
    "\n",
    "    for vector in _idata:\n",
    "        dist = np.sqrt(np.sum(np.power(_weights - vector, 2), axis=1))\n",
    "        c = heapq.nsmallest(factor, range(len(dist)), key=dist.__getitem__)\n",
    "        if (approach==0): # normalized\n",
    "            for j in range(factor):  sdh_m[c[j]] += (factor-j)/cs \n",
    "        if (approach==1):# based on distance\n",
    "            for j in range(factor): sdh_m[c[j]] += 1.0/dist[c[j]] \n",
    "        if (approach==2): \n",
    "            dmin, dmax = min(dist[c]), max(dist[c])\n",
    "            for j in range(factor): sdh_m[c[j]] += 1.0 - (dist[c[j]]-dmin)/(dmax-dmin)\n",
    "\n",
    "    return sdh_m.reshape(_m, _n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c32c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "hv.extension('bokeh')\n",
    "\n",
    "hithist = hv.Image(HitHist(weights['ydim'], weights['ydim'], weights['arr'], idata['arr'])).opts(xaxis=None, yaxis=None) \n",
    "um = hv.Image(UMatrix(weights['ydim'], weights['ydim'], weights['arr'], 4)).opts(xaxis=None, yaxis=None) \n",
    "sdh = hv.Image(SDH(weights['ydim'], weights['ydim'], weights['arr'], idata['arr'], 25, 0)).opts(xaxis=None, yaxis=None)   \n",
    "\n",
    "hv.Layout([hithist.relabel('HitHist').opts(cmap='kr'), \n",
    "           um.relabel('U-Matrix').opts(cmap='jet'), sdh.relabel('SDH').opts(cmap='viridis')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644651b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minisom\n",
    "\n",
    "class AlignedSomLayer(minisom.MiniSom):\n",
    "    \"\"\"\n",
    "    This class implements one layer of an aligned som.\n",
    "    The key difference to the usual MiniSom is that it won't train on its own,\n",
    "    but its training is directed by a parent (AlignedSom) using the update - scalded function.\n",
    "    \"\"\"\n",
    "    def __init__(self, n, i, featuresA, x, y, input_len, sigma=1.0, learning_rate=0.5,\n",
    "                 decay_function=minisom.asymptotic_decay,\n",
    "                 neighborhood_function='gaussian', topology='rectangular',\n",
    "                 activation_distance='euclidean', random_seed=42):\n",
    "        \"\"\"\n",
    "        n: number of layers\n",
    "        i: index of the layer\n",
    "        featuresA: indices of the features that are in the A dataset\n",
    "        all the other arguments get passed to the mini som init\n",
    "        \"\"\"\n",
    "        super().__init__(x, y, input_len, sigma, learning_rate, decay_function, neighborhood_function, topology, activation_distance, random_seed)\n",
    "        self.n = n\n",
    "        self.i = i\n",
    "        self.featuresA = featuresA\n",
    "        \n",
    "        self.underyling_activation_function = self._activation_distance\n",
    "        self._activation_distance = self.activation_scale_wrapper\n",
    "        \n",
    "    def activation_scale_wrapper(self, x, w):\n",
    "        \"\"\"\n",
    "        Wrapper around the activation function of the underlying som.\n",
    "        Scales a vector x and the weights w by the factor given by i and w,\n",
    "        then uses the same activation function as the underlying som would use.\n",
    "        \"\"\"\n",
    "        n = self.n\n",
    "        i = self.i\n",
    "        scaleA = (n-i-1)/(n-1)\n",
    "        scaleB = i/(n-1)\n",
    "\n",
    "        factor = []\n",
    "        for j in range(self._input_len):\n",
    "            if j in self.featuresA:\n",
    "                factor.append(scaleA)\n",
    "            else:\n",
    "                factor.append(scaleB)\n",
    "        \n",
    "        xcopy = x.copy()\n",
    "        wcopy = w.copy()\n",
    "\n",
    "        xcopy *= np.array(factor)\n",
    "        wcopy *= np.array(factor)\n",
    "\n",
    "        return self.underyling_activation_function(xcopy, wcopy)\n",
    "\n",
    "\n",
    "    def update_scaled(self, x, win, t, max_iteration, scaling_factor):\n",
    "        \"\"\"\n",
    "        Updates the weights of the neurons. Scaled by the scaling Factor.\n",
    "        In practice, this factor can be something like 1 / how far the layer is from the \"pivot\" layer\n",
    "        x : np.array\n",
    "            Current pattern to learn.\n",
    "        win : tuple\n",
    "            Position of the winning neuron for x (array or tuple).\n",
    "        t : int\n",
    "            rate of decay for sigma and learning rate\n",
    "        max_iteration : int\n",
    "            If use_epochs is True:\n",
    "                Number of epochs the SOM will be trained for\n",
    "            If use_epochs is False:\n",
    "                Maximum number of iterations (one iteration per sample).\n",
    "        scaling_factor: float\n",
    "            factor the update is scaled by\n",
    "        \"\"\"\n",
    "        eta = self._decay_function(self._learning_rate, t, max_iteration)\n",
    "        # sigma and learning rate decrease with the same rule\n",
    "        sig = self._decay_function(self._sigma, t, max_iteration)\n",
    "        # improves the performances\n",
    "        g = self.neighborhood(win, sig)*eta*scaling_factor\n",
    "        # w_new = eta * neighborhood_function * (x-w)\n",
    "        self._weights += np.einsum('ij, ijk->ijk', g, x-self._weights)\n",
    "\n",
    "    # def update_3d(self, x, win, t, max_iteration, chosen_layer_index):\n",
    "    #     \"\"\"Updates the weights of the neurons.\n",
    "\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     x : np.array\n",
    "    #         Current pattern to learn.\n",
    "    #     win : tuple\n",
    "    #         Position of the winning neuron for x (array or tuple).\n",
    "    #     t : int\n",
    "    #         rate of decay for sigma and learning rate\n",
    "    #     max_iteration : int\n",
    "    #         If use_epochs is True:\n",
    "    #             Number of epochs the SOM will be trained for\n",
    "    #         If use_epochs is False:\n",
    "    #             Maximum number of iterations (one iteration per sample).\n",
    "    #     \"\"\"\n",
    "    #     win_3d = win\n",
    "\n",
    "\n",
    "    #     eta = self._decay_function(self._learning_rate, t, max_iteration)\n",
    "    #     # sigma and learning rate decrease with the same rule\n",
    "    #     sig = self._decay_function(self._sigma, t, max_iteration)\n",
    "    #     # improves the performances\n",
    "    #     g = self.neighborhood(win, sig)*eta\n",
    "    #     # w_new = eta * neighborhood_function * (x-w)\n",
    "    #     self._weights += np.einsum('ij, ijk->ijk', g, x-self._weights)\n",
    "\n",
    "    # def _cosine_distance(self, x, w):\n",
    "    #     num = (w * x).sum(axis=2)\n",
    "    #     denum = multiply(linalg.norm(w, axis=2), linalg.norm(x))\n",
    "    #     return 1 - num / (denum+1e-8)\n",
    "\n",
    "    # def _euclidean_distance(self, x, w):\n",
    "    #     return linalg.norm(subtract(x, w), axis=-1)\n",
    "\n",
    "    # def _manhattan_distance(self, x, w):\n",
    "    #     return linalg.norm(subtract(x, w), ord=1, axis=-1)\n",
    "\n",
    "    # def _chebyshev_distance(self, x, w):\n",
    "    #     return max(subtract(x, w), axis=-1)\n",
    "\n",
    "\n",
    "# import minisom\n",
    "class AlignedSom:\n",
    "    def __init__(self, x, y, input_len, n, featuresA, base_scaling_factor=1.0, sigma=1.0, learning_rate=0.5,\n",
    "                 decay_function=minisom.asymptotic_decay,\n",
    "                 neighborhood_function='gaussian', topology='rectangular',\n",
    "                 activation_distance='euclidean', random_seed=42):\n",
    "        \"\"\"\n",
    "        initializes the Aligned Som class.\n",
    "        n guides how many maps are generated\n",
    "        featuresA sets which features are in the A set (The rest is in B). This should be a list of indices < input_len\n",
    "\n",
    "        All the other params get handed over to the \"children soms\"\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.featuresA = featuresA\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self._random_seed = random_seed\n",
    "\n",
    "        self._learning_rate = learning_rate\n",
    "        self._sigma = sigma\n",
    "        self._input_len = input_len\n",
    "\n",
    "        self.topology = topology\n",
    "\n",
    "        self._decay_function = decay_function\n",
    "        \n",
    "        self._random_generator = np.random.RandomState(random_seed)\n",
    "\n",
    "        self._layers = [AlignedSomLayer(n, i, featuresA, x, y, input_len, sigma=sigma, learning_rate=learning_rate, \n",
    "                                        decay_function=decay_function, \n",
    "                                        neighborhood_function=neighborhood_function, topology=topology, \n",
    "                                        activation_distance=activation_distance, random_seed=random_seed\n",
    "                                        ) for i in range(n)]\n",
    "        \n",
    "        self.base_scaling_factor = base_scaling_factor\n",
    "        self._weights = [self._layers[i]._weights for i in range(n)]\n",
    "\n",
    "\n",
    "    def train(self, data, num_iteration,\n",
    "              random_order=False, verbose=False, use_epochs=False):\n",
    "        \"\"\"\n",
    "        Trains the SOM.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.array or list\n",
    "            Data matrix.\n",
    "\n",
    "        num_iteration : int\n",
    "            If use_epochs is False, the weights will be\n",
    "            updated num_iteration times. Otherwise they will be updated\n",
    "            len(data)*num_iteration times.\n",
    "\n",
    "        random_order : bool (default=False)\n",
    "            If True, samples are picked in random order.\n",
    "            Otherwise the samples are picked sequentially.\n",
    "\n",
    "        verbose : bool (default=False)\n",
    "            If True the status of the training will be\n",
    "            printed each time the weights are updated.\n",
    "\n",
    "        use_epochs : bool (default=False)\n",
    "            If True the SOM will be trained for num_iteration epochs.\n",
    "            In one epoch the weights are updated len(data) times and\n",
    "            the learning rate is constat throughout a single epoch.\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "        # self._check_iteration_number(num_iteration)\n",
    "        # self._check_input_len(data)\n",
    "\n",
    "        random_generator = None\n",
    "        if random_order:\n",
    "            random_generator = self._random_generator\n",
    "        iterations = minisom._build_iteration_indexes(len(data), num_iteration,\n",
    "                                              verbose, random_generator,\n",
    "                                              use_epochs)\n",
    "        if use_epochs:\n",
    "            def get_decay_rate(iteration_index, data_len):\n",
    "                return int(iteration_index / data_len)\n",
    "        else:\n",
    "            def get_decay_rate(iteration_index, data_len):\n",
    "                return int(iteration_index)\n",
    "\n",
    "        for t, iteration in enumerate(iterations):\n",
    "            #picks a random layer\n",
    "            chosen_layer_index = self._random_generator.randint(low=0, high=self.n)\n",
    "            decay_rate = get_decay_rate(t, len(data))\n",
    "            \n",
    "            cur_training_sample = data[iteration]\n",
    "            #calculates winner (a pair of indices)\n",
    "            winner = self._layers[chosen_layer_index].winner(cur_training_sample)\n",
    "\n",
    "\n",
    "            for update_layer_index in range(self.n):\n",
    "                #calculates the distance to the current layer\n",
    "                diff = chosen_layer_index - update_layer_index\n",
    "                if diff < 0:\n",
    "                    diff = -diff\n",
    "\n",
    "                scaling_factor = self.base_scaling_factor/(1+diff)\n",
    "                self._layers[update_layer_index].update_scaled(cur_training_sample, winner,\n",
    "                        decay_rate, num_iteration, scaling_factor)\n",
    "                # self._layers[update_layer_index].update_3d(cur_training_sample, winner,\n",
    "                #         decay_rate, num_iteration, chosen_layer_index)\n",
    "                \n",
    "        \n",
    "        # for t, iteration in enumerate(iterations):\n",
    "        #     decay_rate = get_decay_rate(t, len(data))\n",
    "        #     self.update(data[iteration], self.winner(data[iteration]),\n",
    "        #                 decay_rate, num_iteration)\n",
    "        if verbose:\n",
    "            print('\\n quantization error:', self.quantization_error(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a00733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignedSom = AlignedSom(2, 2, 10, 11, [0, 1, 2])\n",
    "\n",
    "data = [[1]*10,[2]*10]\n",
    "alignedSom.train(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f875a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlignedSomVis():\n",
    "    def __init__(self, weights, input_data, chosen_visulization_index=0):\n",
    "        \"\"\"Initializes an Aligned SOM Visulization object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : list\n",
    "            a list of arrays containing the weights of the SOM layers.\n",
    "\n",
    "        input_data: np.array\n",
    "            the array containing the data vectors\n",
    "\n",
    "        chosen_visulization_index : int\n",
    "            the index for the default visulization shown by _mainview.\n",
    "            Possible values: 0 for Hit Histogram, 1 for U-matrix or 2 for SDH.\n",
    "            Default value is 0.\n",
    "        \"\"\"\n",
    "        self._idata = input_data\n",
    "        self._num_layers = len(weights)\n",
    "        \n",
    "        xdim = weights[0].shape[0]\n",
    "        ydim = weights[0].shape[1]\n",
    "        self._weights = [np.reshape(weights[i], (xdim*ydim, -1)) for i in range(self._num_layers)]\n",
    "        \n",
    "        vector_dim = len(self._idata[0])\n",
    "        \n",
    "        self._images_HitHist = [hv.Image(HitHist(xdim, ydim, self._weights[i], self._idata)).opts(xaxis=None, yaxis=None) for i in range(self._num_layers)]\n",
    "        self._images_UMatrix = [hv.Image(UMatrix(xdim, ydim, self._weights[i], vector_dim)).opts(xaxis=None, yaxis=None) for i in range(self._num_layers)]\n",
    "        self._images_SDH = [hv.Image(SDH(xdim, ydim, self._weights[i], self._idata, 25, 0)).opts(xaxis=None, yaxis=None) for i in range(self._num_layers)]\n",
    "        \n",
    "        self._maps_SDH = [i for i in range(self._num_layers)]\n",
    "        sdh_paramd = [lambda factor, approach: hv.Image(SDH(xdim, ydim, self._weights[i], self._idata, factor, approach)).opts(xaxis=None, yaxis=None) for i in range(self._num_layers)]\n",
    "\n",
    "        self._maps_SDH[0] =  hv.HoloMap({(factor, approach): sdh_paramd[0](factor, approach) for factor in [i+1 for i in range(100)] for approach in [0, 1, 2]},  kdims=['factor', 'approach'])\n",
    "        for i in range(1, self._num_layers):\n",
    "            self._maps_SDH[i] = hv.DynamicMap(sdh_paramd[i], kdims=['factor', 'approach'])\n",
    "        ## NOTE: could not get the code below to run, so used the holomap above as a base\n",
    "        # for dmap in self._maps_SDH:                                                   \n",
    "        #     dmap.redim.values(factor=[i+1 for i in range(100)], approach=[0, 1, 2])\n",
    "\n",
    "        self._visualizations = [self._images_HitHist, self._images_UMatrix, self._maps_SDH]\n",
    "        self._default_color_map = ['kr', 'jet', 'viridis']\n",
    "        self._titles = ['Hit Histogram', 'U-matrix', 'Smoothed Data Histograms']\n",
    "\n",
    "        implemented_vis_number = len(self._visualizations)\n",
    "\n",
    "        self._available_visulizations = [hv.Layout([self._visualizations[j][i].relabel(f'Layer {i}').opts(cmap=self._default_color_map[j]) for i in range(self._num_layers)]).opts(title=self._titles[j]) for j in range(implemented_vis_number)]\n",
    "        self._mainview = self._available_visulizations[chosen_visulization_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7031b2f",
   "metadata": {},
   "source": [
    "C) Evaluation Report\n",
    "1) Perform and document (!) the testing of the components you coded by defining and\n",
    "evaluating suitable tests to evaluate the correctness and robustness of the coded modules.\n",
    "\n",
    "2) For systematic evaluation of tasks a-h, pick the Chainlink Data Set and the 10-Clusters\n",
    "dataset from\n",
    "http://www.ifs.tuwien.ac.at/dm/somtoolbox/datasets.html \t\t\t\t\t\t--> already available in pysomvis datasets\n",
    "\n",
    "3) Train a 10x10 (small) and a 100x60 (large) SOM. Make sure that the SOMs are properly\n",
    "trained, i.e. that the structures to be expected in the SOM become clearly visible by identifying\n",
    "suitable parameters for the initial neighborhood radius and initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d73aec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here\n",
    "from minisom import MiniSom\n",
    "# from pysomvis import PySOMVis #\n",
    "\n",
    "idata_chainlink   = SOMToolBox_Parse(\"datasets/chainlink/chainlink.vec\").read_weight_file()\n",
    "idata_10clusters   = SOMToolBox_Parse(\"datasets/10clusters/10clusters.vec\").read_weight_file()\n",
    "\n",
    "A_set_chainlink = [0, 1]\n",
    "A_set_10clusters = [0, 2, 4, 6, 8]\n",
    "\n",
    "num_layers_chainlink = 10\n",
    "num_layers_10clusters = 10\n",
    "\n",
    "small_som_chainlink = AlignedSom(10, 10, 3, num_layers_chainlink, A_set_chainlink, sigma=7, learning_rate=0.7)\n",
    "# small_som_chainlink = MiniSom(10, 10, 3, sigma=7, learning_rate=0.7)\n",
    "small_som_chainlink.train(idata_chainlink['arr'], 10000)\n",
    "# large_som_chainlink = MiniSom(100, 60, 3, sigma=7, learning_rate=0.7)\n",
    "# large_som_chainlink.train(idata_chainlink['arr'], 10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8036d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = AlignedSomVis(weights=small_som_chainlink._weights, input_data=idata_chainlink['arr'])\n",
    "vis._mainview\n",
    "vis._available_visulizations[2]\n",
    "# vis = AlignedSomVisFirstAttempt(weights=small_som_chainlink._weights, input_data=idata_chainlink['arr'])\n",
    "# vis._mainview\n",
    "# vis = PySOMVis(weights=small_som_chainlink._weights, input_data=idata_chainlink['arr'])\n",
    "# vis._mainview\n",
    "# vis = PySOMVis(weights=large_som_chainlink._weights, input_data=idata_chainlink['arr'])\n",
    "# vis._mainview\n",
    "\n",
    "# small_som_10clusters = MiniSom(10, 10, 10, learning_rate=0.7)\n",
    "# small_som_10clusters.train(idata_10clusters['arr'], 1000)\n",
    "# large_som_10clusters = MiniSom(100, 60, 10, learning_rate=0.7)\n",
    "# large_som_10clusters.train(idata_10clusters['arr'], 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85af3ed7",
   "metadata": {},
   "source": [
    "4) Show the visualizations, providing examples with different parameter settings and\n",
    "comparisons that allow a validation of the correctness of the implementation. Specifically,\n",
    "test a few extreme values for the parameter settings.\n",
    "\n",
    "5) Where an identical visualization exists in the JÃVA SOM toolbox, read a SOM pre-trained\n",
    "with the JAVA SOM Toolbox (import functions are provided in the notebook) and compare\n",
    "your visualization with the one produced by the Java SOMToolbox (using either the pre\n",
    "trained SOMs provided with the toolbox, or any that your colleagues who do the analytics\n",
    "option of the exercise share with you). --> aligned SOM not part of JAVA SOM Toolbox\n",
    "\n",
    "6) Provide (export/print) the notebook as separate PDF report that comprises all information.\n",
    "Hence, the PDF export of the report needs to contain the fully-computed notebook with the\n",
    "according visualizations shown as results and the information that can be derived from the\n",
    "visualizations clearly described and semantically analyzed. Make sure that each visualization\n",
    "includes the parameter setting applied. Specifically, the PDF export needs to contain:\n",
    "- the implementation developed, explaining key parts of the code of each cell.\n",
    "- the way the code was systematically tested for correctness, including the test cases\n",
    "as part of the notebook.\n",
    "- the evaluations performed under item 3) above, demonstrating the correctness of the\n",
    "implementation, and the information gained.\n",
    "- Where applicable: Comparison of the visualization with the identical visualizations\n",
    "(reading the same trained SOM files) using the SOM Java Toolbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53774817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#probably some more code\n",
    "!which xelatex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
