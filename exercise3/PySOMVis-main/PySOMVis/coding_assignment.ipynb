{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c7cf75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pdcoding\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a7c286f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOMToolbox Parser\n",
    "from SOMToolBox_Parse import SOMToolBox_Parse\n",
    "idata = SOMToolBox_Parse(\"datasets/iris/iris.vec\").read_weight_file()\n",
    "weights = SOMToolBox_Parse(\"datasets/iris/iris.wgt.gz\").read_weight_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "259a4e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HitHistogram\n",
    "def HitHist(_m, _n, _weights, _idata):\n",
    "    hist = np.zeros(_m * _n)\n",
    "    for vector in _idata: \n",
    "        position =np.argmin(np.sqrt(np.sum(np.power(_weights - vector, 2), axis=1)))\n",
    "        hist[position] += 1\n",
    "\n",
    "    return hist.reshape(_m, _n)\n",
    "\n",
    "#U-Matrix - implementation\n",
    "def UMatrix(_m, _n, _weights, _dim):\n",
    "    U = _weights.reshape(_m, _n, _dim)\n",
    "    U = np.insert(U, np.arange(1, _n), values=0, axis=1)\n",
    "    U = np.insert(U, np.arange(1, _m), values=0, axis=0)\n",
    "    #calculate interpolation\n",
    "    for i in range(U.shape[0]): \n",
    "        if i%2==0:\n",
    "            for j in range(1,U.shape[1],2):\n",
    "                U[i,j][0] = np.linalg.norm(U[i,j-1] - U[i,j+1], axis=-1)\n",
    "        else:\n",
    "            for j in range(U.shape[1]):\n",
    "                if j%2==0: \n",
    "                    U[i,j][0] = np.linalg.norm(U[i-1,j] - U[i+1,j], axis=-1)\n",
    "                else:      \n",
    "                    U[i,j][0] = (np.linalg.norm(U[i-1,j-1] - U[i+1,j+1], axis=-1) + np.linalg.norm(U[i+1,j-1] - U[i-1,j+1], axis=-1))/(2*np.sqrt(2))\n",
    "\n",
    "    U = np.sum(U, axis=2) #move from Vector to Scalar\n",
    "\n",
    "    for i in range(0, U.shape[0], 2): #count new values\n",
    "        for j in range(0, U.shape[1], 2):\n",
    "            region = []\n",
    "            if j>0: region.append(U[i][j-1]) #check left border\n",
    "            if i>0: region.append(U[i-1][j]) #check bottom\n",
    "            if j<U.shape[1]-1: region.append(U[i][j+1]) #check right border\n",
    "            if i<U.shape[0]-1: region.append(U[i+1][j]) #check upper border\n",
    "\n",
    "            U[i,j] = np.median(region)\n",
    "\n",
    "    return U\n",
    "\n",
    "#SDH - implementation\n",
    "def SDH(_m, _n, _weights, _idata, factor, approach):\n",
    "    import heapq\n",
    "\n",
    "    sdh_m = np.zeros( _m * _n)\n",
    "\n",
    "    cs=0\n",
    "    for i in range(factor): cs += factor-i\n",
    "\n",
    "    for vector in _idata:\n",
    "        dist = np.sqrt(np.sum(np.power(_weights - vector, 2), axis=1))\n",
    "        c = heapq.nsmallest(factor, range(len(dist)), key=dist.__getitem__)\n",
    "        if (approach==0): # normalized\n",
    "            for j in range(factor):  sdh_m[c[j]] += (factor-j)/cs \n",
    "        if (approach==1):# based on distance\n",
    "            for j in range(factor): sdh_m[c[j]] += 1.0/dist[c[j]] \n",
    "        if (approach==2): \n",
    "            dmin, dmax = min(dist[c]), max(dist[c])\n",
    "            for j in range(factor): sdh_m[c[j]] += 1.0 - (dist[c[j]]-dmin)/(dmax-dmin)\n",
    "\n",
    "    return sdh_m.reshape(_m, _n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c32c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "hv.extension('bokeh')\n",
    "\n",
    "hithist = hv.Image(HitHist(weights['ydim'], weights['ydim'], weights['arr'], idata['arr'])).opts(xaxis=None, yaxis=None) \n",
    "um = hv.Image(UMatrix(weights['ydim'], weights['ydim'], weights['arr'], 4)).opts(xaxis=None, yaxis=None) \n",
    "sdh = hv.Image(SDH(weights['ydim'], weights['ydim'], weights['arr'], idata['arr'], 25, 0)).opts(xaxis=None, yaxis=None)   \n",
    "\n",
    "hv.Layout([hithist.relabel('HitHist').opts(cmap='kr'), \n",
    "           um.relabel('U-Matrix').opts(cmap='jet'), sdh.relabel('SDH').opts(cmap='viridis')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "644651b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minisom\n",
    "\n",
    "class AlignedSomLayer(minisom.MiniSom):\n",
    "    def __init__(self, n, i, featuresA, x, y, input_len, sigma=1.0, learning_rate=0.5,\n",
    "                 decay_function=minisom.asymptotic_decay,\n",
    "                 neighborhood_function='gaussian', topology='rectangular',\n",
    "                 activation_distance='euclidean', random_seed=42):\n",
    "        super().__init__(x, y, input_len, sigma, learning_rate, decay_function, neighborhood_function, topology, activation_distance, random_seed)\n",
    "        self.n = n\n",
    "        self.i = i\n",
    "        self.featuresA = featuresA\n",
    "        \n",
    "        self.underyling_activation_function = self._activation_distance\n",
    "        self._activation_distance = self.activation_scale_wrapper\n",
    "        \n",
    "        #self.rescale(featuresA, n, i)\n",
    "\n",
    "    def activation_scale_wrapper(self, x, w):\n",
    "        # print(x)\n",
    "        n = self.n\n",
    "        i = self.i\n",
    "        scaleA = (n-i-1)/(n-1)\n",
    "        scaleB = i/(n-1)\n",
    "\n",
    "        factor = []\n",
    "        for j in range(self._input_len):\n",
    "            if j in self.featuresA:\n",
    "                factor.append(scaleA)\n",
    "            else:\n",
    "                factor.append(scaleB)\n",
    "        \n",
    "        xcopy = x.copy()\n",
    "        wcopy = w.copy()\n",
    "\n",
    "        # print(f\"layer {i}\")\n",
    "        # print(\"old\")\n",
    "        # print(xcopy)\n",
    "        # print(wcopy)\n",
    "\n",
    "        xcopy *= np.array(factor)\n",
    "        wcopy *= np.array(factor)\n",
    "\n",
    "\n",
    "        # print(\"new\")\n",
    "        # print(xcopy)\n",
    "        # print(wcopy)\n",
    "\n",
    "        return self.underyling_activation_function(xcopy, wcopy)\n",
    "\n",
    "\n",
    "\n",
    "    def rescale(self, featuresA, n, i):\n",
    "        scaleA = (n-i-1)/(n-1)\n",
    "        scaleB = i/(n-1)\n",
    "\n",
    "        factor = []\n",
    "        for i in range(self._input_len):\n",
    "            if i in featuresA:\n",
    "                factor.append(scaleA)\n",
    "            else:\n",
    "                factor.append(scaleB)\n",
    "\n",
    "        self._weights *= factor\n",
    "\n",
    "\n",
    "    def update_scaled(self, x, win, t, max_iteration, scaling_factor):\n",
    "        \"\"\"\n",
    "        Updates the weights of the neurons. Scaled by the scaling Factor.\n",
    "        In practice, this factor can be something like 1 / how far the layer is from the \"pivot\" layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            Current pattern to learn.\n",
    "        win : tuple\n",
    "            Position of the winning neuron for x (array or tuple).\n",
    "        t : int\n",
    "            rate of decay for sigma and learning rate\n",
    "        max_iteration : int\n",
    "            If use_epochs is True:\n",
    "                Number of epochs the SOM will be trained for\n",
    "            If use_epochs is False:\n",
    "                Maximum number of iterations (one iteration per sample).\n",
    "        scaling_factor: float\n",
    "            factor the update is scaled by\n",
    "        \"\"\"\n",
    "        eta = self._decay_function(self._learning_rate, t, max_iteration)\n",
    "        # sigma and learning rate decrease with the same rule\n",
    "        sig = self._decay_function(self._sigma, t, max_iteration)\n",
    "        # improves the performances\n",
    "        g = self.neighborhood(win, sig)*eta*scaling_factor\n",
    "        # w_new = eta * neighborhood_function * (x-w)\n",
    "        self._weights += np.einsum('ij, ijk->ijk', g, x-self._weights)\n",
    "\n",
    "    # def update_3d(self, x, win, t, max_iteration, chosen_layer_index):\n",
    "    #     \"\"\"Updates the weights of the neurons.\n",
    "\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     x : np.array\n",
    "    #         Current pattern to learn.\n",
    "    #     win : tuple\n",
    "    #         Position of the winning neuron for x (array or tuple).\n",
    "    #     t : int\n",
    "    #         rate of decay for sigma and learning rate\n",
    "    #     max_iteration : int\n",
    "    #         If use_epochs is True:\n",
    "    #             Number of epochs the SOM will be trained for\n",
    "    #         If use_epochs is False:\n",
    "    #             Maximum number of iterations (one iteration per sample).\n",
    "    #     \"\"\"\n",
    "    #     win_3d = win\n",
    "\n",
    "\n",
    "    #     eta = self._decay_function(self._learning_rate, t, max_iteration)\n",
    "    #     # sigma and learning rate decrease with the same rule\n",
    "    #     sig = self._decay_function(self._sigma, t, max_iteration)\n",
    "    #     # improves the performances\n",
    "    #     g = self.neighborhood(win, sig)*eta\n",
    "    #     # w_new = eta * neighborhood_function * (x-w)\n",
    "    #     self._weights += np.einsum('ij, ijk->ijk', g, x-self._weights)\n",
    "\n",
    "    # def _cosine_distance(self, x, w):\n",
    "    #     num = (w * x).sum(axis=2)\n",
    "    #     denum = multiply(linalg.norm(w, axis=2), linalg.norm(x))\n",
    "    #     return 1 - num / (denum+1e-8)\n",
    "\n",
    "    # def _euclidean_distance(self, x, w):\n",
    "    #     return linalg.norm(subtract(x, w), axis=-1)\n",
    "\n",
    "    # def _manhattan_distance(self, x, w):\n",
    "    #     return linalg.norm(subtract(x, w), ord=1, axis=-1)\n",
    "\n",
    "    # def _chebyshev_distance(self, x, w):\n",
    "    #     return max(subtract(x, w), axis=-1)\n",
    "\n",
    "\n",
    "# import minisom\n",
    "class AlignedSom:\n",
    "    def __init__(self, x, y, input_len, n, featuresA, sigma=1.0, learning_rate=0.5,\n",
    "                 decay_function=minisom.asymptotic_decay,\n",
    "                 neighborhood_function='gaussian', topology='rectangular',\n",
    "                 activation_distance='euclidean', random_seed=42):\n",
    "        \"\"\"\n",
    "        initializes the Aligned Som class.\n",
    "        n guides how many maps are generated\n",
    "        featuresA sets which features are in the A set (The rest is in B). This should be a list of indices < input_len\n",
    "\n",
    "        All the other params get handed over to the \"children soms\"\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.featuresA = featuresA\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self._random_seed = random_seed\n",
    "\n",
    "        self._learning_rate = learning_rate\n",
    "        self._sigma = sigma\n",
    "        self._input_len = input_len\n",
    "\n",
    "        self.topology = topology\n",
    "\n",
    "        self._decay_function = decay_function\n",
    "        \n",
    "        self._random_generator = np.random.RandomState(random_seed)\n",
    "\n",
    "        self._layers = [AlignedSomLayer(n, i, featuresA, x, y, input_len, sigma=sigma, learning_rate=learning_rate, \n",
    "                                        decay_function=decay_function, \n",
    "                                        neighborhood_function=neighborhood_function, topology=topology, \n",
    "                                        activation_distance=activation_distance, random_seed=random_seed\n",
    "                                        ) for i in range(n)]\n",
    "        \n",
    "        \n",
    "        self._weights = [self._layers[i]._weights for i in range(n)]\n",
    "\n",
    "    def get_scaled_vector(self, x, i):\n",
    "        scaleA = (self.n - 1 - i)/(self.n-1)\n",
    "        scaleB = i/(self.n-1)\n",
    "\n",
    "        xnew = x.copy()\n",
    "        \n",
    "        for j in range(len(x)):\n",
    "            if j in self.featuresA:\n",
    "                xnew[j] *= scaleA\n",
    "            else:\n",
    "                xnew[j] *= scaleB\n",
    "        \n",
    "        return xnew\n",
    "\n",
    "    def train(self, data, num_iteration,\n",
    "              random_order=False, verbose=False, use_epochs=False):\n",
    "        \"\"\"\n",
    "        Trains the SOM.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.array or list\n",
    "            Data matrix.\n",
    "\n",
    "        num_iteration : int\n",
    "            If use_epochs is False, the weights will be\n",
    "            updated num_iteration times. Otherwise they will be updated\n",
    "            len(data)*num_iteration times.\n",
    "\n",
    "        random_order : bool (default=False)\n",
    "            If True, samples are picked in random order.\n",
    "            Otherwise the samples are picked sequentially.\n",
    "\n",
    "        verbose : bool (default=False)\n",
    "            If True the status of the training will be\n",
    "            printed each time the weights are updated.\n",
    "\n",
    "        use_epochs : bool (default=False)\n",
    "            If True the SOM will be trained for num_iteration epochs.\n",
    "            In one epoch the weights are updated len(data) times and\n",
    "            the learning rate is constat throughout a single epoch.\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "        # self._check_iteration_number(num_iteration)\n",
    "        # self._check_input_len(data)\n",
    "\n",
    "        random_generator = None\n",
    "        if random_order:\n",
    "            random_generator = self._random_generator\n",
    "        iterations = minisom._build_iteration_indexes(len(data), num_iteration,\n",
    "                                              verbose, random_generator,\n",
    "                                              use_epochs)\n",
    "        if use_epochs:\n",
    "            def get_decay_rate(iteration_index, data_len):\n",
    "                return int(iteration_index / data_len)\n",
    "        else:\n",
    "            def get_decay_rate(iteration_index, data_len):\n",
    "                return int(iteration_index)\n",
    "\n",
    "        for t, iteration in enumerate(iterations):\n",
    "            chosen_layer_index = self._random_generator.randint(low=0, high=self.n)\n",
    "            decay_rate = get_decay_rate(t, len(data))\n",
    "            \n",
    "            cur_training_sample = data[iteration]\n",
    "            winner = self._layers[chosen_layer_index].winner(cur_training_sample)\n",
    "\n",
    "\n",
    "            for update_layer_index in range(self.n):\n",
    "                diff = chosen_layer_index - update_layer_index\n",
    "                if diff < 0:\n",
    "                    diff = -diff\n",
    "                \n",
    "                scaling_factor = 1/(1+diff)\n",
    "                self._layers[update_layer_index].update_scaled(cur_training_sample, winner,\n",
    "                        decay_rate, num_iteration, scaling_factor)\n",
    "                # self._layers[update_layer_index].update_3d(cur_training_sample, winner,\n",
    "                #         decay_rate, num_iteration, chosen_layer_index)\n",
    "                \n",
    "        \n",
    "        # for t, iteration in enumerate(iterations):\n",
    "        #     decay_rate = get_decay_rate(t, len(data))\n",
    "        #     self.update(data[iteration], self.winner(data[iteration]),\n",
    "        #                 decay_rate, num_iteration)\n",
    "        if verbose:\n",
    "            print('\\n quantization error:', self.quantization_error(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a00733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignedSom = AlignedSom(2, 2, 10, 11, [0, 1, 2])\n",
    "\n",
    "data = [[1]*10,[2]*10]\n",
    "alignedSom.train(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8f875a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlignedSomVis():\n",
    "    def __init__(self, weights, input_data, chosen_visulization_index=0):\n",
    "        \"\"\"Initializes an Aligned SOM Visulization object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : list\n",
    "            a list of arrays containing the weights of the SOM layers.\n",
    "\n",
    "        input_data: np.array\n",
    "            the array containing the data vectors\n",
    "\n",
    "        chosen_visulization_index : int\n",
    "            the index for the default visulization shown by _mainview.\n",
    "            Possible values: 0 for Hit Histogram, 1 for U-matrix or 2 for SDH.\n",
    "            Default value is 0.\n",
    "        \"\"\"\n",
    "        self._idata = input_data\n",
    "        self._num_layers = len(weights)\n",
    "        \n",
    "        xdim = weights[0].shape[0]\n",
    "        ydim = weights[0].shape[1]\n",
    "        self._weights = [np.reshape(weights[i], (xdim*ydim, -1)) for i in range(self._num_layers)]\n",
    "        \n",
    "        vector_dim = len(self._idata[0])\n",
    "        \n",
    "        self._images_HitHist = [hv.Image(HitHist(xdim, ydim, self._weights[i], self._idata)).opts(xaxis=None, yaxis=None) for i in range(self._num_layers)]\n",
    "        self._images_UMatrix = [hv.Image(UMatrix(xdim, ydim, self._weights[i], vector_dim)).opts(xaxis=None, yaxis=None) for i in range(self._num_layers)]\n",
    "        self._images_SDH = [hv.Image(SDH(xdim, ydim, self._weights[i], self._idata, 25, 0)).opts(xaxis=None, yaxis=None) for i in range(self._num_layers)]\n",
    "        \n",
    "        self._maps_SDH = [i for i in range(self._num_layers)]\n",
    "        sdh_paramd = [lambda factor, approach: hv.Image(SDH(xdim, ydim, self._weights[i], self._idata, factor, approach)).opts(xaxis=None, yaxis=None) for i in range(self._num_layers)]\n",
    "\n",
    "        self._maps_SDH[0] =  hv.HoloMap({(factor, approach): sdh_paramd[0](factor, approach) for factor in [i+1 for i in range(100)] for approach in [0, 1, 2]},  kdims=['factor', 'approach']) #might need to disable this for larger SOMs if it takes too long\n",
    "        for i in range(1, self._num_layers):\n",
    "            self._maps_SDH[i] = hv.DynamicMap(sdh_paramd[i], kdims=['factor', 'approach'])\n",
    "        ## NOTE: could not get the code below to run, so used the holomap above as a base\n",
    "        # for dmap in self._maps_SDH:                                                   \n",
    "        #     dmap.redim.values(factor=[i+1 for i in range(100)], approach=[0, 1, 2])\n",
    "\n",
    "        self._visualizations = [self._images_HitHist, self._images_UMatrix, self._maps_SDH]\n",
    "        self._default_color_map = ['kr', 'jet', 'viridis']\n",
    "        self._titles = ['Hit Histogram', 'U-matrix', 'Smoothed Data Histograms']\n",
    "\n",
    "        implemented_vis_number = len(self._visualizations)\n",
    "\n",
    "        self._available_visulizations = [hv.Layout([self._visualizations[j][i].relabel(f'Layer {i}').opts(cmap=self._default_color_map[j]) for i in range(self._num_layers)]).opts(title=self._titles[j]) for j in range(implemented_vis_number)]\n",
    "        self._mainview = self._available_visulizations[chosen_visulization_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7031b2f",
   "metadata": {},
   "source": [
    "C) Evaluation Report\n",
    "1) Perform and document (!) the testing of the components you coded by defining and\n",
    "evaluating suitable tests to evaluate the correctness and robustness of the coded modules.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25fa16",
   "metadata": {},
   "source": [
    "\n",
    "3) Train a 10x10 (small) and a 100x60 (large) SOM. Make sure that the SOMs are properly\n",
    "trained, i.e. that the structures to be expected in the SOM become clearly visible by identifying\n",
    "suitable parameters for the initial neighborhood radius and initial learning rate.\n",
    "\n",
    "Below, we train a 10x10 (small) and a 100x60 (large) SOM on the Chainlink Data Set and the 10-Clusters\n",
    "Data Set. \n",
    "RE:Learning rate and initial neighborhood radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d73aec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idata_chainlink = SOMToolBox_Parse(\"datasets/chainlink/chainlink.vec\").read_weight_file()\n",
    "\n",
    "A_set_chainlink = [1, 2]\n",
    "\n",
    "num_layers_chainlink = 10\n",
    "\n",
    "small_som_chainlink = AlignedSom(10, 10, 3, num_layers_chainlink, A_set_chainlink, sigma=7, learning_rate=0.7)\n",
    "small_som_chainlink.train(idata_chainlink['arr'], 10000)\n",
    "large_som_chainlink = AlignedSom(100, 60, 3, num_layers_chainlink, A_set_chainlink, sigma=7, learning_rate=0.7)\n",
    "large_som_chainlink.train(idata_chainlink['arr'], 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8036d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_small_som_chainlink = AlignedSomVis(weights=small_som_chainlink._weights, input_data=idata_chainlink['arr'])\n",
    "vis_small_som_chainlink._mainview\n",
    "# vis_small_som_chainlink._available_visulizations[1]\n",
    "# vis_small_som_chainlink._available_visulizations[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80da080",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_large_som_chainlink = AlignedSomVis(weights=large_som_chainlink._weights, input_data=idata_chainlink['arr'])\n",
    "vis_large_som_chainlink._mainview\n",
    "# vis_large_som_chainlink._available_visulizations[1]\n",
    "# vis_large_som_chainlink._available_visulizations[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9782e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "idata_10clusters = SOMToolBox_Parse(\"datasets/10clusters/10clusters.vec\").read_weight_file()\n",
    "\n",
    "A_set_10clusters = [0, 2, 4, 6, 8]\n",
    "num_layers_10clusters = 10\n",
    "\n",
    "small_som_10clusters = AlignedSom(10, 10, 10, num_layers_10clusters, A_set_10clusters, learning_rate=0.7)\n",
    "small_som_10clusters.train(idata_10clusters['arr'], 10000)\n",
    "\n",
    "large_som_10clusters = AlignedSom(100, 60, 10, num_layers_10clusters, A_set_10clusters, learning_rate=0.7)\n",
    "large_som_10clusters.train(idata_10clusters['arr'], 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385cbb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_small_som_10clusters = AlignedSomVis(weights=small_som_10clusters._weights, input_data=idata_10clusters['arr'])\n",
    "vis_small_som_10clusters._mainview\n",
    "# vis_small_som_10clusters._available_visulizations[1]\n",
    "# vis_small_som_10clusters._available_visulizations[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b0382",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_large_som_10clusters = AlignedSomVis(weights=large_som_10clusters._weights, input_data=idata_10clusters['arr'])\n",
    "vis_large_som_10clusters._mainview\n",
    "# vis_large_som_10clusters._available_visulizations[1]\n",
    "# vis_large_som_10clusters._available_visulizations[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85af3ed7",
   "metadata": {},
   "source": [
    "4) Show the visualizations, providing examples with different parameter settings and\n",
    "comparisons that allow a validation of the correctness of the implementation. Specifically,\n",
    "test a few extreme values for the parameter settings.\n",
    "\n",
    "6) Provide (export/print) the notebook as separate PDF report that comprises all information.\n",
    "Hence, the PDF export of the report needs to contain the fully-computed notebook with the\n",
    "according visualizations shown as results and the information that can be derived from the\n",
    "visualizations clearly described and semantically analyzed. Make sure that each visualization\n",
    "includes the parameter setting applied. Specifically, the PDF export needs to contain:\n",
    "- the implementation developed, explaining key parts of the code of each cell.\n",
    "- the way the code was systematically tested for correctness, including the test cases\n",
    "as part of the notebook.\n",
    "- the evaluations performed under item 3) above, demonstrating the correctness of the\n",
    "implementation, and the information gained."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOS_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "title": "Aligned SOM",
  "authors": [
  { "name": "Haubenburger Gabriel 11840531" },
  { "name": "David Seka 11902064" }
]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
